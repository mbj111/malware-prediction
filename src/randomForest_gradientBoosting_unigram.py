# -*- coding: utf-8 -*-
"""
Training Random Forest and Gradient Boosting classifier on unigram features.

"""
import os
import numpy as np
import gzip
from csv import reader, writer
from sklearn.ensemble import RandomForestClassifier
from sklearn import feature_selection 
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.svm import  LinearSVC
import six
#Initialize feature selction object
fs = feature_selection.SelectPercentile(feature_selection.chi2, percentile=20) 

# Decide read/write mode based on python version
read_mode, write_mode = ('r','w') if six.PY2 else ('rt','wt')

# Decide zip based on python version
if six.PY2:
    from itertools import izip
    zp = izip
else:
    zp = zip

# Set path to your consolidated files
path ='/media/admin-19/Seagate Expansion Drive/NS PROJECT Data/Unigram'
os.chdir(path)

# File names
ftrain = 'train_consolidation.gz'
ftest = 'test_consolidation.gz'
flabel = 'trainLabels.csv'
fsubmission = 'submission.gz'

print('loading started')
# Lets read labels first as things are not sorted in files
labels = {}
with open(flabel) as f:
    next(f)    # Ignoring header
    for row in reader(f):
        labels[row[0]] = int(row[1])
print('labels loaded')

# Dimensions for train set
ntrain = 10868
nfeature = 16**2 + 1 + 1 # For two_byte_codes, no_que_marks, label
train = np.zeros((ntrain, nfeature), dtype = int)
with gzip.open(ftrain, read_mode) as f:
    next(f)    # Ignoring header
    for t,row in enumerate(reader(f)):
        train[t,:-1] = map(int, row[1:]) if six.PY2 else list(map(int, row[1:]))
        train[t,-1] = labels[row[0]]
        if(t+1)%1000==0:
            print(t+1, 'records loaded')
print('training set loaded')
# Training data collected here, Feature extraction needs to be done here  X=train[t,:-1] , Y=train[t,-1]
del labels
X_train = fs.fit_transform(train[:,:-1],train[:,-1])
scores = fs.scores_ 

#Dictionary with format feature_index[i]=j means ith feature has j significance
feature_index={}
i=0
for score in scores:
	feature_index[i]=score
	i=i+1
#Features contains the index of the features with highest relavance to the model . Use these features in constructing the test dataset
Features =  (sorted(feature_index, key=feature_index.__getitem__, reverse=True))

	
# Parameters for Randomforest
random_state = 123
n_jobs = 5
verbose = 2
clf = RandomForestClassifier(n_estimators= 200,random_state=random_state, n_jobs=n_jobs, verbose = verbose)
clf1= GradientBoostingClassifier(n_estimators=300,random_state=random_state, verbose=verbose)

# Start training
print('training started')
clf1.fit(X_train, train[:,-1])
print('training completed')

# We don't need training set now
del train

# Dimensions for train set
ntest = 10873
nfeature = 16**2 + 1 # For two_byte_codes, no_que_marks
test = np.zeros((ntest, nfeature), dtype = int)
Ids = []    # Required test set ids

with gzip.open(ftest, read_mode) as f:
    next(f)    # Ignoring header
    for t,row in enumerate(reader(f)):
        test[t,:] = map(int, row[1:]) if six.PY2 else list(map(int, row[1:]))
        Ids.append(row[0])
        if(t+1)%1000==0:
            print(t+1, 'records loaded')
print('test set loaded')

#Select features found in the training step
X_test_list=[]
j=0
for i in Features and j <51 :
	X_test_list.append(X_train[:,i])
	j=j+1

X_testT=np.array(X_test_list)
X_test=np.transpose(X_testT)

# Predict for whole test set
y_pred = clf1.predict_proba(X_test)

# Writing results to file
with gzip.open(fsubmission, write_mode) as f:
    fw = writer(f)
    # Header preparation
    header = ['Id'] + ['Prediction'+str(i) for i in range(1,10)]
    fw.writerow(header)
    for t, (Id, pred) in enumerate(zp(Ids, y_pred.tolist())):
        fw.writerow([Id]+pred)
        if(t+1)%1000==0:
            print(t+1, 'prediction written')
